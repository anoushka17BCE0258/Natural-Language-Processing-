{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "line=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist=list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\hp\\\\Desktop\\\\nlp.txt']\n"
     ]
    }
   ],
   "source": [
    "flist = glob.glob(r'C:\\Users\\hp\\Desktop\\nlp.txt')\n",
    "print(flist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTS began its formation in 2010 after Big Hit Entertainment CEO Bang Si-hyuk met with group leader RM and was impressed with his rapping.[9] BTS was originally supposed to be a hip-hop group similar to YG Entertainment's 1TYM,[10] but between their initial formation and their debut, Bang Si-hyuk decided that the contemporary youth needed instead \"a hero who can lend them a shoulder to lean on, even without speaking a single word\".[11] The group was meant to debut in 2011 and featured on several tracks by artists such as 2AM and Lee Seung-gi before their debut was postponed and the group was reorganized into a more traditional idol group.[12] The lineup was then finalized with Jin, Suga, J-Hope, RM, Jimin, V, and Jungkook in 2012. Six months prior to their debut, they began to gain attention for their presence on various social media websites, as well as song covers on YouTube and SoundCloud.\n",
      "['bts began its formation in 2010 after big hit entertainment ceo bang si-hyuk met with group leader rm and was impressed with his rapping.[9] bts was originally supposed to be a hip-hop group similar to yg entertainment\\'s 1tym,[10] but between their initial formation and their debut, bang si-hyuk decided that the contemporary youth needed instead \"a hero who can lend them a shoulder to lean on, even without speaking a single word\".[11] the group was meant to debut in 2011 and featured on several tracks by artists such as 2am and lee seung-gi before their debut was postponed and the group was reorganized into a more traditional idol group.[12] the lineup was then finalized with jin, suga, j-hope, rm, jimin, v, and jungkook in 2012. six months prior to their debut, they began to gain attention for their presence on various social media websites, as well as song covers on youtube and soundcloud.']\n"
     ]
    }
   ],
   "source": [
    "wordlist = list()\n",
    "s = set()\n",
    "for fname in flist:\n",
    "    tfile = open(fname, 'r')\n",
    "    line = tfile.read()\n",
    "    print(line)\n",
    "    line = line.lower()\n",
    "    #s = s.union(set(line.split(' ')))\n",
    "    wordlist.append(line)\n",
    "    tfile.close()\n",
    "#print(s)\n",
    "print(wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens=[]\n",
    "for doc in wordlist:\n",
    "    word_tokens.append(word_tokenize(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2=[]\n",
    "for doc in word_tokens:\n",
    "    new_word_tokens=[]\n",
    "    for word in doc:\n",
    "        if word not in stop_words:\n",
    "            new_word_tokens.append(ps.stem(word))\n",
    "    l2.append( new_word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=set()\n",
    "for doc in l2:\n",
    "    s=s.union(set(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(s)\n",
    "s=list(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf=dict()\n",
    "for term in s:\n",
    "    c=0\n",
    "    for doc in l2:\n",
    "        if term in doc:\n",
    "            c+=1;\n",
    "    idf[term]=c/3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf=[]\n",
    "for doc in l2:\n",
    "    di=dict()\n",
    "    max=0\n",
    "    for terms in s:\n",
    "        di[terms]=0\n",
    "        for word in doc:\n",
    "            if word==terms:\n",
    "                di[terms]+=1\n",
    "        if(max<=di[terms]):\n",
    "            max=di[terms]\n",
    "    for t in di:\n",
    "        di[t]=(di[t]/max)*idf[t]\n",
    "    tfidf.append(di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.06060606060606061,\n",
       " 0.12121212121212122,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.06060606060606061,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.06060606060606061,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.06060606060606061,\n",
       " 0.030303030303030304,\n",
       " 0.06060606060606061,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.12121212121212122,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.06060606060606061,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.12121212121212122,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.3333333333333333,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.1515151515151515,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.12121212121212122,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304,\n",
       " 0.06060606060606061,\n",
       " 0.030303030303030304,\n",
       " 0.030303030303030304]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tfidf[0].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str='bts began its formation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens=word_tokenize(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=[]\n",
    "for word in word_tokens:\n",
    "    if word not in set(stopwords.words('english')):\n",
    "        n.append(word)\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pstem=[]\n",
    "for word in word_tokens:\n",
    "    if word not in set(stopwords.words('english')):\n",
    "        n_pstem.append(ps.stem(word))\n",
    "n_pstem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = dict()\n",
    "for word in n:\n",
    "    if word in tf:\n",
    "        tf[word]+=1\n",
    "    else:\n",
    "        tf[word]=1\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf2=[]\n",
    "m=0\n",
    "for terms in s:\n",
    "    di[terms]=0\n",
    "    for word in n_pstem:\n",
    "        if word==terms:\n",
    "            di[terms]+=1\n",
    "        if(m<=di[terms]):\n",
    "            m=di[terms]\n",
    "for t in di:\n",
    "    di[t]=(di[t]/m)*idf[t]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    " \n",
    "# vectors\n",
    "a = np.array(list(tfidf[0].values()))\n",
    "b = np.array(list(di.values()))\n",
    " \n",
    "# manually compute cosine similarity\n",
    "dot = np.dot(a, b)\n",
    "norma = np.linalg.norm(a)\n",
    "normb = np.linalg.norm(b)\n",
    "cos = dot / (norma * normb)\n",
    " \n",
    "# cos_lib = cosine_similarity(aa, ba)\n",
    " \n",
    "# print(\n",
    "#     dot,\n",
    "#     norma,\n",
    "#     normb,\n",
    "#     cos,\n",
    "#     cos_lib[0][0]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    " \n",
    "# vectors\n",
    "a = np.array(list(tfidf[1].values()))\n",
    "b = np.array(list(di.values()))\n",
    " \n",
    "# manually compute cosine similarity\n",
    "dot = np.dot(a, b)\n",
    "norma = np.linalg.norm(a)\n",
    "normb = np.linalg.norm(b)\n",
    "cos = dot / (norma * normb)\n",
    " \n",
    "# cos_lib = cosine_similarity(aa, ba)\n",
    " \n",
    "# print(\n",
    "#     dot,\n",
    "#     norma,\n",
    "#     normb,\n",
    "#     cos,\n",
    "#     cos_lib[0][0]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    " \n",
    "# vectors\n",
    "a = np.array(list(tfidf[2].values()))\n",
    "b = np.array(list(di.values()))\n",
    " \n",
    "# manually compute cosine similarity\n",
    "dot = np.dot(a, b)\n",
    "norma = np.linalg.norm(a)\n",
    "normb = np.linalg.norm(b)\n",
    "cos = dot / (norma * normb)\n",
    " \n",
    "# cos_lib = cosine_similarity(aa, ba)\n",
    " \n",
    "# print(\n",
    "#     dot,\n",
    "#     norma,\n",
    "#     normb,\n",
    "#     cos,\n",
    "#     cos_lib[0][0]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
