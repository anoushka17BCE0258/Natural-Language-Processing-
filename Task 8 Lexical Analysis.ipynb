{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords,CMU wordList,WordNet, Pipelining, Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['au',\n",
       " 'aux',\n",
       " 'avec',\n",
       " 'ce',\n",
       " 'ces',\n",
       " 'dans',\n",
       " 'de',\n",
       " 'des',\n",
       " 'du',\n",
       " 'elle',\n",
       " 'en',\n",
       " 'et',\n",
       " 'eux',\n",
       " 'il',\n",
       " 'ils',\n",
       " 'je',\n",
       " 'la',\n",
       " 'le',\n",
       " 'les',\n",
       " 'leur',\n",
       " 'lui',\n",
       " 'ma',\n",
       " 'mais',\n",
       " 'me',\n",
       " 'm√™me',\n",
       " 'mes',\n",
       " 'moi',\n",
       " 'mon',\n",
       " 'ne',\n",
       " 'nos',\n",
       " 'notre',\n",
       " 'nous',\n",
       " 'on',\n",
       " 'ou',\n",
       " 'par',\n",
       " 'pas',\n",
       " 'pour',\n",
       " 'qu',\n",
       " 'que',\n",
       " 'qui',\n",
       " 'sa',\n",
       " 'se',\n",
       " 'ses',\n",
       " 'son',\n",
       " 'sur',\n",
       " 'ta',\n",
       " 'te',\n",
       " 'tes',\n",
       " 'toi',\n",
       " 'ton',\n",
       " 'tu',\n",
       " 'un',\n",
       " 'une',\n",
       " 'vos',\n",
       " 'votre',\n",
       " 'vous',\n",
       " 'c',\n",
       " 'd',\n",
       " 'j',\n",
       " 'l',\n",
       " '√†',\n",
       " 'm',\n",
       " 'n',\n",
       " 's',\n",
       " 't',\n",
       " 'y',\n",
       " '√©t√©',\n",
       " '√©t√©e',\n",
       " '√©t√©es',\n",
       " '√©t√©s',\n",
       " '√©tant',\n",
       " '√©tante',\n",
       " '√©tants',\n",
       " '√©tantes',\n",
       " 'suis',\n",
       " 'es',\n",
       " 'est',\n",
       " 'sommes',\n",
       " '√™tes',\n",
       " 'sont',\n",
       " 'serai',\n",
       " 'seras',\n",
       " 'sera',\n",
       " 'serons',\n",
       " 'serez',\n",
       " 'seront',\n",
       " 'serais',\n",
       " 'serait',\n",
       " 'serions',\n",
       " 'seriez',\n",
       " 'seraient',\n",
       " '√©tais',\n",
       " '√©tait',\n",
       " '√©tions',\n",
       " '√©tiez',\n",
       " '√©taient',\n",
       " 'fus',\n",
       " 'fut',\n",
       " 'f√ªmes',\n",
       " 'f√ªtes',\n",
       " 'furent',\n",
       " 'sois',\n",
       " 'soit',\n",
       " 'soyons',\n",
       " 'soyez',\n",
       " 'soient',\n",
       " 'fusse',\n",
       " 'fusses',\n",
       " 'f√ªt',\n",
       " 'fussions',\n",
       " 'fussiez',\n",
       " 'fussent',\n",
       " 'ayant',\n",
       " 'ayante',\n",
       " 'ayantes',\n",
       " 'ayants',\n",
       " 'eu',\n",
       " 'eue',\n",
       " 'eues',\n",
       " 'eus',\n",
       " 'ai',\n",
       " 'as',\n",
       " 'avons',\n",
       " 'avez',\n",
       " 'ont',\n",
       " 'aurai',\n",
       " 'auras',\n",
       " 'aura',\n",
       " 'aurons',\n",
       " 'aurez',\n",
       " 'auront',\n",
       " 'aurais',\n",
       " 'aurait',\n",
       " 'aurions',\n",
       " 'auriez',\n",
       " 'auraient',\n",
       " 'avais',\n",
       " 'avait',\n",
       " 'avions',\n",
       " 'aviez',\n",
       " 'avaient',\n",
       " 'eut',\n",
       " 'e√ªmes',\n",
       " 'e√ªtes',\n",
       " 'eurent',\n",
       " 'aie',\n",
       " 'aies',\n",
       " 'ait',\n",
       " 'ayons',\n",
       " 'ayez',\n",
       " 'aient',\n",
       " 'eusse',\n",
       " 'eusses',\n",
       " 'e√ªt',\n",
       " 'eussions',\n",
       " 'eussiez',\n",
       " 'eussent']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('French')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CMU Word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133737"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "ent=nltk.corpus.cmudict.entries()\n",
    "len(ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('belford', ['B', 'EH1', 'L', 'F', 'ER0', 'D'])\n",
      "('belfry', ['B', 'EH1', 'L', 'F', 'R', 'IY0'])\n",
      "('belgacom', ['B', 'EH1', 'L', 'G', 'AH0', 'K', 'AA0', 'M'])\n",
      "('belgacom', ['B', 'EH1', 'L', 'JH', 'AH0', 'K', 'AA0', 'M'])\n",
      "('belgard', ['B', 'EH0', 'L', 'G', 'AA1', 'R', 'D'])\n",
      "('belgarde', ['B', 'EH0', 'L', 'G', 'AA1', 'R', 'D', 'IY0'])\n",
      "('belge', ['B', 'EH1', 'L', 'JH', 'IY0'])\n",
      "('belger', ['B', 'EH1', 'L', 'G', 'ER0'])\n",
      "('belgian', ['B', 'EH1', 'L', 'JH', 'AH0', 'N'])\n",
      "('belgians', ['B', 'EH1', 'L', 'JH', 'AH0', 'N', 'Z'])\n",
      "('belgique', ['B', 'EH0', 'L', 'ZH', 'IY1', 'K'])\n",
      "(\"belgique's\", ['B', 'EH0', 'L', 'JH', 'IY1', 'K', 'S'])\n",
      "('belgium', ['B', 'EH1', 'L', 'JH', 'AH0', 'M'])\n",
      "(\"belgium's\", ['B', 'EH1', 'L', 'JH', 'AH0', 'M', 'Z'])\n",
      "('belgo', ['B', 'EH1', 'L', 'G', 'OW2'])\n",
      "('belgrade', ['B', 'EH1', 'L', 'G', 'R', 'EY0', 'D'])\n",
      "('belgrade', ['B', 'EH1', 'L', 'G', 'R', 'AA2', 'D'])\n",
      "(\"belgrade's\", ['B', 'EH1', 'L', 'G', 'R', 'EY0', 'D', 'Z'])\n",
      "(\"belgrade's\", ['B', 'EH1', 'L', 'G', 'R', 'AA2', 'D', 'Z'])\n",
      "('belgrave', ['B', 'EH1', 'L', 'G', 'R', 'EY2', 'V'])\n",
      "('beli', ['B', 'EH1', 'L', 'IY0'])\n",
      "('belich', ['B', 'EH1', 'L', 'IH0', 'K'])\n",
      "('belie', ['B', 'IH0', 'L', 'AY1'])\n",
      "('belied', ['B', 'IH0', 'L', 'AY1', 'D'])\n",
      "('belief', ['B', 'IH0', 'L', 'IY1', 'F'])\n"
     ]
    }
   ],
   "source": [
    "for i in ent[10000:10025]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', ['AH0'])\n",
      "('a.', ['EY1'])\n",
      "('a', ['EY1'])\n",
      "('a42128', ['EY1', 'F', 'AO1', 'R', 'T', 'UW1', 'W', 'AH1', 'N', 'T', 'UW1', 'EY1', 'T'])\n",
      "('aaa', ['T', 'R', 'IH2', 'P', 'AH0', 'L', 'EY1'])\n",
      "('aaberg', ['AA1', 'B', 'ER0', 'G'])\n",
      "('aachen', ['AA1', 'K', 'AH0', 'N'])\n",
      "('aachener', ['AA1', 'K', 'AH0', 'N', 'ER0'])\n",
      "('aaker', ['AA1', 'K', 'ER0'])\n",
      "('aalseth', ['AA1', 'L', 'S', 'EH0', 'TH'])\n",
      "('aamodt', ['AA1', 'M', 'AH0', 'T'])\n",
      "('aancor', ['AA1', 'N', 'K', 'AO2', 'R'])\n",
      "('aardema', ['AA0', 'R', 'D', 'EH1', 'M', 'AH0'])\n",
      "('aardvark', ['AA1', 'R', 'D', 'V', 'AA2', 'R', 'K'])\n",
      "('aaron', ['EH1', 'R', 'AH0', 'N'])\n",
      "(\"aaron's\", ['EH1', 'R', 'AH0', 'N', 'Z'])\n",
      "('aarons', ['EH1', 'R', 'AH0', 'N', 'Z'])\n",
      "('aaronson', ['EH1', 'R', 'AH0', 'N', 'S', 'AH0', 'N'])\n",
      "('aaronson', ['AA1', 'R', 'AH0', 'N', 'S', 'AH0', 'N'])\n",
      "(\"aaronson's\", ['EH1', 'R', 'AH0', 'N', 'S', 'AH0', 'N', 'Z'])\n",
      "(\"aaronson's\", ['AA1', 'R', 'AH0', 'N', 'S', 'AH0', 'N', 'Z'])\n",
      "('aarti', ['AA1', 'R', 'T', 'IY2'])\n",
      "('aase', ['AA1', 'S'])\n",
      "('aasen', ['AA1', 'S', 'AH0', 'N'])\n",
      "('ab', ['AE1', 'B'])\n",
      "('ab', ['EY1', 'B', 'IY1'])\n",
      "('ababa', ['AH0', 'B', 'AA1', 'B', 'AH0'])\n",
      "('ababa', ['AA1', 'B', 'AH0', 'B', 'AH0'])\n",
      "('abacha', ['AE1', 'B', 'AH0', 'K', 'AH0'])\n",
      "('aback', ['AH0', 'B', 'AE1', 'K'])\n",
      "('abaco', ['AE1', 'B', 'AH0', 'K', 'OW2'])\n",
      "('abacus', ['AE1', 'B', 'AH0', 'K', 'AH0', 'S'])\n",
      "('abad', ['AH0', 'B', 'AA1', 'D'])\n",
      "('abadaka', ['AH0', 'B', 'AE1', 'D', 'AH0', 'K', 'AH0'])\n",
      "('abadi', ['AH0', 'B', 'AE1', 'D', 'IY0'])\n",
      "('abadie', ['AH0', 'B', 'AE1', 'D', 'IY0'])\n",
      "('abair', ['AH0', 'B', 'EH1', 'R'])\n",
      "('abalkin', ['AH0', 'B', 'AA1', 'L', 'K', 'IH0', 'N'])\n",
      "('abalone', ['AE2', 'B', 'AH0', 'L', 'OW1', 'N', 'IY0'])\n",
      "('abalos', ['AA0', 'B', 'AA1', 'L', 'OW0', 'Z'])\n",
      "('abandon', ['AH0', 'B', 'AE1', 'N', 'D', 'AH0', 'N'])\n",
      "('abandoned', ['AH0', 'B', 'AE1', 'N', 'D', 'AH0', 'N', 'D'])\n",
      "('abandoning', ['AH0', 'B', 'AE1', 'N', 'D', 'AH0', 'N', 'IH0', 'NG'])\n",
      "('abandonment', ['AH0', 'B', 'AE1', 'N', 'D', 'AH0', 'N', 'M', 'AH0', 'N', 'T'])\n",
      "('abandonments', ['AH0', 'B', 'AE1', 'N', 'D', 'AH0', 'N', 'M', 'AH0', 'N', 'T', 'S'])\n",
      "('abandons', ['AH0', 'B', 'AE1', 'N', 'D', 'AH0', 'N', 'Z'])\n",
      "('abanto', ['AH0', 'B', 'AE1', 'N', 'T', 'OW0'])\n",
      "('abarca', ['AH0', 'B', 'AA1', 'R', 'K', 'AH0'])\n",
      "('abare', ['AA0', 'B', 'AA1', 'R', 'IY0'])\n",
      "('abascal', ['AE1', 'B', 'AH0', 'S', 'K', 'AH0', 'L'])\n"
     ]
    }
   ],
   "source": [
    "for i in ent[0:50]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('car.n.01')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "wn.synsets('motorcar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car', 'auto', 'automobile', 'machine', 'motorcar']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('sludge.n.02'),\n",
       " Synset('droppings.n.01'),\n",
       " Synset('muck.v.01'),\n",
       " Synset('manure.v.01'),\n",
       " Synset('mire.v.04')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('muck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['droppings', 'dung', 'muck']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('droppings.n.01').lemma_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "II nltk pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('BTS', 'NNP'), ('(', '('), ('Korean', 'JJ'), (':', ':'), ('Î∞©ÌÉÑÏÜåÎÖÑÎã®', 'NN'), (';', ':'), ('RR', 'NNP'), (':', ':'), ('Bangtan', 'NNP'), ('Sonyeondan', 'NNP'), (')', ')'), (',', ','), ('also', 'RB'), ('known', 'VBN'), ('as', 'IN'), ('the', 'DT'), ('Bangtan', 'NNP'), ('Boys', 'NNP'), (',', ','), ('is', 'VBZ'), ('a', 'DT'), ('seven-member', 'JJ'), ('South', 'JJ'), ('Korean', 'JJ'), ('boy', 'NN'), ('band', 'NN'), ('formed', 'VBN'), ('in', 'IN'), ('Seoul', 'NNP'), ('in', 'IN'), ('2013', 'CD'), ('.', '.')]\n",
      "[('The', 'DT'), ('septet', 'JJ'), ('co-writes', 'NNS'), ('and', 'CC'), ('produces', 'VBZ'), ('much', 'RB'), ('of', 'IN'), ('their', 'PRP$'), ('output', 'NN'), ('.', '.')]\n",
      "[('Originally', 'RB'), ('a', 'DT'), ('hip', 'NN'), ('hop', 'NN'), ('group', 'NN'), (',', ','), ('their', 'PRP$'), ('musical', 'JJ'), ('style', 'NN'), ('has', 'VBZ'), ('evolved', 'VBN'), ('to', 'TO'), ('include', 'VB'), ('a', 'DT'), ('wide', 'JJ'), ('range', 'NN'), ('of', 'IN'), ('genres', 'NNS'), ('.', '.')]\n",
      "[('Their', 'PRP$'), ('lyrics', 'NNS'), (',', ','), ('often', 'RB'), ('focused', 'VBN'), ('on', 'IN'), ('personal', 'JJ'), ('and', 'CC'), ('social', 'JJ'), ('commentary', 'NN'), (',', ','), ('touch', 'NN'), ('on', 'IN'), ('the', 'DT'), ('themes', 'NNS'), ('of', 'IN'), ('mental', 'JJ'), ('health', 'NN'), (',', ','), ('troubles', 'NNS'), ('of', 'IN'), ('school-age', 'NN'), ('youth', 'NN'), (',', ','), ('loss', 'NN'), (',', ','), ('the', 'DT'), ('journey', 'NN'), ('towards', 'IN'), ('loving', 'VBG'), ('oneself', 'PRP'), (',', ','), ('and', 'CC'), ('individualism', 'NN'), ('.', '.')]\n",
      "[('Their', 'PRP$'), ('work', 'NN'), ('features', 'NNS'), ('references', 'NNS'), ('to', 'TO'), ('literature', 'NN'), ('and', 'CC'), ('psychological', 'JJ'), ('concepts', 'NNS'), ('and', 'CC'), ('includes', 'VBZ'), ('an', 'DT'), ('alternative', 'JJ'), ('universe', 'JJ'), ('storyline', 'NN'), ('.', '.')]\n",
      "[('The', 'DT'), ('group', 'NN'), ('have', 'VBP'), ('staged', 'VBN'), ('several', 'JJ'), ('world', 'NN'), ('tours', 'NNS'), ('.', '.')]\n",
      "[('BTS', 'NNP'), (\"'\", 'POS'), ('lyrics', 'NNS'), ('include', 'VBP'), ('social', 'JJ'), ('commentary', 'NN'), (',', ','), ('often', 'RB'), ('incorporating', 'VBG'), ('criticism', 'NN'), ('of', 'IN'), ('South', 'JJ'), ('Korean', 'JJ'), ('society', 'NN'), ('.', '.')]\n",
      "[('Songs', 'NNS'), ('such', 'JJ'), ('as', 'IN'), ('``', '``'), ('No', 'NNP'), ('More', 'NNP'), ('Dream', 'NNP'), (\"''\", \"''\"), ('and', 'CC'), ('``', '``'), ('N.O', 'NNP'), (\"''\", \"''\"), ('from', 'IN'), ('their', 'PRP$'), ('``', '``'), ('school', 'NN'), ('trilogy', 'NN'), (\"''\", \"''\"), ('were', 'VBD'), ('motivated', 'VBN'), ('by', 'IN'), ('their', 'PRP$'), ('experiences', 'NNS'), ('with', 'IN'), ('South', 'NNP'), ('Korea', 'NNP'), (\"'s\", 'POS'), ('emphasis', 'NN'), ('on', 'IN'), ('education', 'NN'), ('and', 'CC'), ('called', 'VBD'), ('for', 'IN'), ('change', 'NN'), ('to', 'TO'), ('the', 'DT'), ('education', 'NN'), ('system', 'NN'), ('and', 'CC'), ('societal', 'JJ'), ('expectations', 'NNS'), ('.', '.')]\n",
      "[('Their', 'PRP$'), ('experiences', 'NNS'), ('with', 'IN'), ('youth', 'JJ'), ('culture', 'NN'), ('in', 'IN'), ('South', 'NNP'), ('Korea', 'NNP'), ('inspired', 'VBD'), ('songs', 'NNS'), ('like', 'IN'), ('``', '``'), ('Dope', 'NNP'), (\"''\", \"''\"), ('and', 'CC'), ('``', '``'), ('Silver', 'NNP'), ('Spoon', 'NNP'), (\"''\", \"''\"), ('(', '('), ('Korean', 'JJ'), (':', ':'), ('Î±ÅÏÉà', 'NN'), (';', ':'), ('RR', 'NNP'), (':', ':'), ('Baepsae', 'NNP'), (')', ')'), ('from', 'IN'), ('their', 'PRP$'), ('``', '``'), ('youth', 'JJ'), ('series', 'NN'), (',', ','), (\"''\", \"''\"), ('referencing', 'VBG'), ('generational', 'JJ'), ('disparity', 'NN'), ('and', 'CC'), ('the', 'DT'), ('millennial', 'NN'), (\"'s\", 'POS'), ('giving', 'VBG'), ('up', 'IN'), ('of', 'IN'), ('romantic', 'JJ'), ('relationships', 'NNS'), (',', ','), ('marriage', 'NN'), (',', ','), ('children', 'NNS'), (',', ','), ('proper', 'JJ'), ('employment', 'NN'), (',', ','), ('homes', 'NNS'), (',', ','), ('and', 'CC'), ('social', 'JJ'), ('life', 'NN'), ('in', 'IN'), ('the', 'DT'), ('face', 'NN'), ('of', 'IN'), ('economic', 'JJ'), ('difficulties', 'NNS'), ('and', 'CC'), ('societal', 'JJ'), ('ills', 'NNS'), ('while', 'IN'), ('facing', 'VBG'), ('condemnation', 'NN'), ('from', 'IN'), ('the', 'DT'), ('media', 'NNS'), ('and', 'CC'), ('older', 'JJR'), ('generations', 'NNS'), ('.', '.')]\n",
      "[('The', 'DT'), ('song', 'NN'), ('``', '``'), ('Am', 'NNP'), ('I', 'PRP'), ('Wrong', 'JJ'), (\"''\", \"''\"), ('from', 'IN'), ('Wings', 'NNP'), ('(', '('), ('2016', 'CD'), (')', ')'), ('questioned', 'VBD'), ('societal', 'JJ'), ('apathy', 'JJ'), ('towards', 'IN'), ('the', 'DT'), ('state', 'NN'), ('of', 'IN'), ('current', 'JJ'), ('events‚Äîthe', 'NN'), ('lyric', 'JJ'), ('``', '``'), ('We', 'PRP'), (\"'re\", 'VBP'), ('all', 'DT'), ('dogs', 'NNS'), ('and', 'CC'), ('pigs', 'NNS'), ('/', 'VBP'), ('we', 'PRP'), ('become', 'VBP'), ('dogs', 'NNS'), ('because', 'IN'), ('we', 'PRP'), (\"'re\", 'VBP'), ('angry', 'JJ'), (\"''\", \"''\"), ('referenced', 'VBD'), ('the', 'DT'), ('South', 'JJ'), ('Korean', 'JJ'), ('Ministry', 'NNP'), ('of', 'IN'), ('Education', 'NNP'), ('official', 'NN'), ('Na', 'NNP'), ('Hyang-wook', 'NNP'), ('who', 'WP'), ('was', 'VBD'), ('a', 'DT'), ('proponent', 'NN'), ('of', 'IN'), ('the', 'DT'), ('caste', 'NN'), ('system', 'NN'), ('and', 'CC'), ('described', 'VBD'), ('the', 'DT'), ('average', 'JJ'), ('person', 'NN'), ('as', 'IN'), ('``', '``'), ('dogs', 'NNS'), ('and', 'CC'), ('pigs', 'NNS'), (',', ','), (\"''\", \"''\"), ('and', 'CC'), ('BTS', 'NNP'), ('performed', 'VBD'), ('the', 'DT'), ('song', 'NN'), ('on', 'IN'), ('television', 'NN'), ('during', 'IN'), ('the', 'DT'), ('2016', 'CD'), ('South', 'JJ'), ('Korean', 'JJ'), ('political', 'JJ'), ('scandal', 'NN'), ('that', 'WDT'), ('led', 'VBD'), ('to', 'TO'), ('the', 'DT'), ('impeachment', 'NN'), ('of', 'IN'), ('ex-President', 'JJ'), ('Park', 'NNP'), ('Geun-hye', 'NNP'), ('.', '.')]\n",
      "[('RM', 'NNP'), ('and', 'CC'), ('Suga', 'NNP'), (\"'s\", 'POS'), ('personal', 'JJ'), ('struggles', 'NNS'), ('with', 'IN'), ('mental', 'JJ'), ('health', 'NN'), ('inspired', 'VBD'), ('songs', 'NNS'), ('like', 'IN'), ('``', '``'), ('Tomorrow', 'NNP'), (\"''\", \"''\"), (',', ','), ('``', '``'), ('Intro', 'NN'), (':', ':'), ('The', 'DT'), ('Most', 'RBS'), ('Beautiful', 'JJ'), ('Moment', 'NN'), ('in', 'IN'), ('Life', 'NNP'), (\"''\", \"''\"), (',', ','), ('``', '``'), ('So', 'RB'), ('Far', 'NNP'), ('Away', 'NNP'), (\"''\", \"''\"), (',', ','), ('``', '``'), ('The', 'DT'), ('Last', 'JJ'), (\"''\", \"''\"), (',', ','), ('and', 'CC'), ('``', '``'), ('Forever', 'RB'), ('Rain', 'NNP'), ('.', '.'), (\"''\", \"''\")]\n",
      "[('``', '``'), ('Not', 'RB'), ('Today', 'NNP'), (\"''\", \"''\"), ('from', 'IN'), ('You', 'PRP'), ('Never', 'NNP'), ('Walk', 'VBP'), ('Alone', 'NNP'), ('(', '('), ('2017', 'CD'), (')', ')'), ('is', 'VBZ'), ('an', 'DT'), ('anti-establishment', 'JJ'), ('anthem', 'NN'), (',', ','), ('with', 'IN'), ('messages', 'NNS'), ('advocating', 'VBG'), ('for', 'IN'), ('minority', 'NN'), ('groups', 'NNS'), (',', ','), ('while', 'IN'), ('``', '``'), ('Spring', 'NNP'), ('Day', 'NNP'), (\"''\", \"''\"), ('was', 'VBD'), ('created', 'VBN'), ('to', 'TO'), ('memorialize', 'VB'), ('the', 'DT'), ('victims', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('Sewol', 'NNP'), ('Ferry', 'NNP'), ('tragedy', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "te=[\"\"\"BTS (Korean: Î∞©ÌÉÑÏÜåÎÖÑÎã®; RR: Bangtan Sonyeondan), also known as the Bangtan Boys, is a seven-member South Korean boy band formed in Seoul in 2013. \n",
    "The septet co-writes and produces much of their output. Originally a hip hop group, their musical style has evolved to include a wide range of genres.\n",
    "Their lyrics, often focused on personal and social commentary, touch on the themes of mental health, troubles of school-age youth, loss, the journey towards loving oneself, and individualism. \n",
    "Their work features references to literature and psychological concepts and includes an alternative universe storyline. \n",
    "The group have staged several world tours.\n",
    "BTS' lyrics include social commentary, often incorporating criticism of South Korean society. \n",
    "Songs such as \"No More Dream\" and \"N.O\" from their \"school trilogy\" were motivated by their experiences with South Korea's emphasis on education and called for change to the education system and societal expectations.\n",
    "Their experiences with youth culture in South Korea inspired songs like \"Dope\" and \"Silver Spoon\" (Korean: Î±ÅÏÉà; RR: Baepsae) from their \"youth series,\" referencing generational disparity and the millennial's giving up of romantic relationships, marriage, children, proper employment, homes, and social life in the face of economic difficulties and societal ills while facing condemnation from the media and older generations.\n",
    "The song \"Am I Wrong\" from Wings (2016) questioned societal apathy towards the state of current events‚Äîthe lyric \"We're all dogs and pigs / we become dogs because we're angry\" referenced the South Korean Ministry of Education official Na Hyang-wook who was a proponent of the caste system and described the average person as \"dogs and pigs,\" and BTS performed the song on television during the 2016 South Korean political scandal that led to the impeachment of ex-President Park Geun-hye.\n",
    "RM and Suga's personal struggles with mental health inspired songs like \"Tomorrow\", \"Intro: The Most Beautiful Moment in Life\", \"So Far Away\", \"The Last\", and \"Forever Rain.\"\n",
    "\"Not Today\" from You Never Walk Alone (2017) is an anti-establishment anthem, with messages advocating for minority groups,while \"Spring Day\" was created to memorialize the victims of the Sewol Ferry tragedy.\"\"\"]\n",
    "for i in te:\n",
    "    s=nltk.sent_tokenize(i)\n",
    "    for sen in s:\n",
    "        w=nltk.word_tokenize(sen)\n",
    "        tagged=nltk.pos_tag(w)\n",
    "        print(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "III Impementing tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OMG',\n",
       " 'these',\n",
       " 'are',\n",
       " 'amazing',\n",
       " '!',\n",
       " 'You',\n",
       " 'guys',\n",
       " 'have',\n",
       " 'so',\n",
       " 'much',\n",
       " 'fun',\n",
       " 'together',\n",
       " 'and',\n",
       " 'truly',\n",
       " 'are',\n",
       " 'a',\n",
       " 'light',\n",
       " 'in',\n",
       " 'each',\n",
       " 'other',\n",
       " '‚Äô',\n",
       " 's',\n",
       " 'lives',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'ours',\n",
       " 'üíú',\n",
       " 'ü•≥']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "text='OMG these are amazing! You guys have so much fun together and truly are a light in each other‚Äôs lives as well as ours üíúü•≥'\n",
    "tnew=TweetTokenizer()\n",
    "tnew.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
